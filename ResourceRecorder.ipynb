{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PlatJack/LIDC-IDRI/blob/main/ResourceRecorder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm\n",
        "!pip install kornia\n",
        "!pip install pylidc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GIOVgad3Fmk",
        "outputId": "2532cd2a-b017-4314-916e-c7487ffc5053"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm\n",
            "  Downloading timm-0.9.7-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from timm) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.15.2+cu118)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n",
            "Collecting huggingface-hub (from timm)\n",
            "  Downloading huggingface_hub-0.17.1-py3-none-any.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.8/294.8 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors (from timm)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm) (16.0.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (4.66.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (23.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.23.5)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7->timm) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->timm) (1.3.0)\n",
            "Installing collected packages: safetensors, huggingface-hub, timm\n",
            "Successfully installed huggingface-hub-0.17.1 safetensors-0.3.3 timm-0.9.7\n",
            "Collecting kornia\n",
            "  Downloading kornia-0.7.0-py2.py3-none-any.whl (705 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m705.7/705.7 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from kornia) (23.1)\n",
            "Requirement already satisfied: torch>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from kornia) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.9.1->kornia) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.9.1->kornia) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9.1->kornia) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.9.1->kornia) (1.3.0)\n",
            "Installing collected packages: kornia\n",
            "Successfully installed kornia-0.7.0\n",
            "Collecting pylidc\n",
            "  Downloading pylidc-0.2.3-py2.py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sqlalchemy>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from pylidc) (2.0.20)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from pylidc) (1.23.5)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.10/dist-packages (from pylidc) (1.11.2)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pylidc) (3.7.1)\n",
            "Collecting pydicom>=1.0.0 (from pylidc)\n",
            "  Downloading pydicom-2.4.3-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-image>=0.13 in /usr/local/lib/python3.10/dist-packages (from pylidc) (0.19.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pylidc) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pylidc) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pylidc) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pylidc) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pylidc) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pylidc) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pylidc) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pylidc) (2.8.2)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.13->pylidc) (3.1)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.13->pylidc) (2.31.3)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.13->pylidc) (2023.8.30)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.13->pylidc) (1.4.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.1.5->pylidc) (4.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.1.5->pylidc) (2.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=2.0.0->pylidc) (1.16.0)\n",
            "Installing collected packages: pydicom, pylidc\n",
            "Successfully installed pydicom-2.4.3 pylidc-0.2.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcvpUtE70REq",
        "outputId": "5edf8736-aee8-40c0-fde3-87ea881cd4f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import timm\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR,StepLR\n",
        "from torch.utils.data import Subset\n",
        "import kornia.augmentation as K\n",
        "import torch.multiprocessing\n",
        "torch.multiprocessing.set_sharing_strategy('file_system')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pylidc as pl\n",
        "import math\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "from timm.models.layers import to_2tuple,trunc_normal_,DropPath,SelectAdaptivePool2d\n",
        "from timm.models.convnext import LayerNorm2d\n",
        "\n",
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random"
      ],
      "metadata": {
        "id": "gsM6lfg10r1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cube_from_img(img3d, center, block_size):\n",
        "    center_x = center[0]\n",
        "    center_y = center[1]\n",
        "    center_z = center[2]\n",
        "\n",
        "    block_size_x = block_size[0]\n",
        "    block_size_y = block_size[1]\n",
        "    block_size_z = block_size[2]\n",
        "\n",
        "    start_x = max(center_x - block_size_x / 2, 0)\n",
        "    if start_x + block_size_x > img3d.shape[0]:\n",
        "        start_x = img3d.shape[0] - block_size_x\n",
        "\n",
        "    start_y = max(center_y - block_size_y / 2, 0)\n",
        "    if start_y + block_size_y > img3d.shape[1]:\n",
        "        start_y = img3d.shape[1] - block_size_y\n",
        "\n",
        "    start_z = max(center_z - block_size_z / 2, 0)\n",
        "    if start_z + block_size_z > img3d.shape[2]:\n",
        "        start_z = img3d.shape[2] - block_size_z\n",
        "\n",
        "    start_x = int(start_x)\n",
        "    start_y = int(start_y)\n",
        "    start_z = int(start_z)\n",
        "    roi_img3d = img3d[ start_x:start_x + block_size_x,\n",
        "                      start_y:start_y + block_size_y,\n",
        "                      start_z:start_z + block_size_z]\n",
        "    return roi_img3d\n",
        "\n",
        "data_list = sorted(glob.glob('/content/drive/MyDrive/LIDC-IDRI/LIDC-IDRI/*/*/*'))\n",
        "data_num = len(data_list)\n",
        "nodule_info = []\n",
        "\n",
        "block_size = [64,64,56]\n",
        "\n",
        "save_path_p = 'Data/classification/npyfiles/'\n",
        "k = 0\n",
        "\n",
        "print('------------------------------------------------')\n",
        "for d_idx in range(1, data_num + 1):\n",
        "\n",
        "\n",
        "    pid = data_list[d_idx].split('/')[-1]\n",
        "    print('processing-----{}'.format(pid))\n",
        "    scan = pl.query(pl.Scan).filter(pl.Scan.id == pid).first()\n",
        "    vol = scan.to_volume()\n",
        "    print('------------------------------------------------')\n",
        "    nods = scan.cluster_annotations()\n",
        "    num_nods = len(nods)\n",
        "\n",
        "    sid = scan.series_instance_uid\n",
        "\n",
        "    pixel_info = scan.spacings\n",
        "\n",
        "    for i, nod_i in enumerate(nods):\n",
        "\n",
        "        num_name = str(k)\n",
        "        if k <10 : num_name = '000' + num_name\n",
        "        elif k <100: num_name = '00' + num_name\n",
        "        elif k <1000: num_name = '0' + num_name\n",
        "        save_name = 'Index{}.npy'.format(num_name)\n",
        "\n",
        "        cent = []\n",
        "        diameter = 0\n",
        "        mal_factor = 0\n",
        "        bbox = []\n",
        "        num_ann = len(nod_i)\n",
        "        for j, ann_i in enumerate(nod_i):\n",
        "            cent.append(ann_i.centroid)\n",
        "            diameter += ann_i.diameter\n",
        "            mal_factor += ann_i.feature_vals()[-1]\n",
        "            bbox.append(ann_i.bbox_dims())\n",
        "        cent = np.mean(cent,axis=0)\n",
        "        diameter = diameter/num_ann\n",
        "        bbox = np.max(bbox,axis=0)\n",
        "        mal_factor = mal_factor/num_ann\n",
        "\n",
        "\n",
        "        if mal_factor >3 : mal = 1.\n",
        "        else: mal = 0.\n",
        "\n",
        "\n",
        "        nodule_info.append([pid,sid,*pixel_info,*cent,*bbox,diameter,mal_factor,mal])\n",
        "        patch = get_cube_from_img(vol, cent, block_size)\n",
        "\n",
        "        if patch.shape[0]<block_size[0]: print('error: index {}'.format(save_name))\n",
        "\n",
        "\n",
        "        if mal>0: save_path_f = save_path_p + '/True/'\n",
        "        else: save_path_f = save_path_p + '/False/'\n",
        "\n",
        "        os.makedirs(save_path_f, exist_ok=True)\n",
        "        np.save(save_path_f+save_name,patch)\n",
        "        k +=1\n",
        "\n",
        "column_index =['patient_id','serisuid',\n",
        "               'pixel_x','pixel_y','pixel_z',\n",
        "               'interp_cent_x','interp_cent_y','interp_cent_z',\n",
        "               'bbox_x','bbox_y','bbox_z',\n",
        "               'diameter','malignancy_level','malignancy'\n",
        "               ]\n",
        "nodule_info_csv = pd.DataFrame(np.array(nodule_info),columns=column_index)\n",
        "nodule_info_csv = nodule_info_csv.set_index(column_index[0])\n",
        "nodule_info_csv.to_csv('nodule_info.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "_WOZG-4O8lvZ",
        "outputId": "a1f57069-6c47-47ef-ed42-bca53ed787c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------\n",
            "processing-----3000923.000000-NA-62357\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-f241cf43cb55>\u001b[0m in \u001b[0;36m<cell line: 40>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'processing-----{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mscan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mScan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mScan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mvol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_volume\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'------------------------------------------------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mnods\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_annotations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'to_volume'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(Dataset):\n",
        "    def __init__(self, path, mode ='train'):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        with open('{}/splits/{}.txt'.format(path,mode), 'r') as fin:\n",
        "            data_list = [line.replace('\\n','') for line in fin]\n",
        "\n",
        "\n",
        "        self.img_path_list = sorted(data_list)\n",
        "        self.label_list =  [x.split('/')[-2] for x in self.img_path_list]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = self.img_path_list[index]\n",
        "        label = int(self.label_list[index]=='True')\n",
        "\n",
        "        img = np.load(img_path).astype(float)\n",
        "        img = (img - img.min()) / (img.max() - img.min())\n",
        "        img = torch.Tensor(img.transpose(2,1,0))\n",
        "\n",
        "\n",
        "        return img, label\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_path_list)\n",
        "\n",
        "    def get_labels(self):\n",
        "        return self.label_list"
      ],
      "metadata": {
        "id": "92-soy761Xvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataAugmentation(nn.Module):\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "        self.transforms = nn.Sequential(\n",
        "            K.RandomVerticalFlip(p=0.5),\n",
        "            K.RandomThinPlateSpline(p=0.5),\n",
        "            K.RandomAffine((0,180),p=0.5),\n",
        "            K.RandomPerspective(0.5,p=0.5),\n",
        "            )\n",
        "    @torch.no_grad()\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x_out = self.transforms(x)\n",
        "        return x_out"
      ],
      "metadata": {
        "id": "xA44Wotc1zlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, params):\n",
        "    num_epochs = params['num_epochs']\n",
        "    batch_size = params['batch_size']\n",
        "    optimizer = params['optimizer']\n",
        "    loss_function=params['loss_function']\n",
        "    data_path = params['data_path']\n",
        "    model_path = params['model_path']\n",
        "    norm = params['norm']\n",
        "    l_lambda = params['lambda']\n",
        "    best =0\n",
        "    ds_tr = Dataset(data_path,'train')\n",
        "    ds_val = Dataset(data_path,'val')\n",
        "\n",
        "\n",
        "    dl_tr = DataLoader(ds_tr,\n",
        "                       batch_size=batch_size,\n",
        "                       pin_memory=True,\n",
        "                       shuffle = True,\n",
        "                       num_workers=0)\n",
        "\n",
        "    dl_val = DataLoader(ds_val,\n",
        "                       batch_size=batch_size,\n",
        "                       pin_memory=True,\n",
        "                       shuffle = False,\n",
        "                       num_workers=0)\n",
        "    dl = {'train':dl_tr, 'val': dl_val}\n",
        "\n",
        "\n",
        "    augmentation = DataAugmentation()\n",
        "\n",
        "    scheduler = CosineAnnealingLR(optimizer, T_max=len(dl_tr), eta_min=0, last_epoch=-1)\n",
        "\n",
        "    for epoch in range(0, num_epochs):\n",
        "        with tqdm(dl['train'], unit=\"batch\") as tepoch:\n",
        "            total = 0\n",
        "            correct = 0\n",
        "\n",
        "            model.train()\n",
        "            for data in tepoch:\n",
        "                tepoch.set_description(f\"LR {optimizer.param_groups[0]['lr']},Epoch {epoch}\")\n",
        "\n",
        "                inputs, labels = data\n",
        "                inputs = augmentation(inputs)\n",
        "                inputs = inputs.cuda()\n",
        "                labels = labels.cuda()\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                outputs = model(inputs)\n",
        "\n",
        "                predictions = outputs.argmax(dim=1, keepdim=True).squeeze()\n",
        "                correct += (predictions == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "                accuracy = 100*correct / total\n",
        "\n",
        "                train_loss = loss_function(outputs, labels)\n",
        "\n",
        "                if norm == 1:\n",
        "\n",
        "                    l_norm = torch.norm(torch.cat([p.view(-1) for p in model.parameters()]), p=norm)\n",
        "                    train_loss = train_loss+l_lambda*l_norm\n",
        "                elif norm ==2:\n",
        "\n",
        "                    l_norm = torch.norm(torch.cat([p.view(-1) for p in model.parameters()]), p=norm)\n",
        "                    train_loss = train_loss+l_lambda*l_norm\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                train_loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                scheduler.step()\n",
        "                tepoch.set_postfix(loss=train_loss.item(),accuracy=accuracy)\n",
        "\n",
        "\n",
        "            if (epoch==0) or epoch>15*num_epochs//20:\n",
        "                total = 0\n",
        "                correct = 0\n",
        "                test_loss = 0\n",
        "                accuracy = []\n",
        "                loss =[]\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    for i, data in enumerate(dl['val'], 0):\n",
        "                            inputs, labels = data\n",
        "                            inputs = inputs.cuda()\n",
        "                            labels = labels.cuda()\n",
        "\n",
        "                            outputs = model(inputs)\n",
        "\n",
        "                            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "                            total += labels.size(0)\n",
        "                            correct += (predicted == labels).sum().item()\n",
        "\n",
        "                            test_loss = loss_function(outputs, labels)\n",
        "                            test_loss += test_loss.item()\n",
        "                accuracy.append(100 * correct/total)\n",
        "                loss.append(100 *test_loss/total)\n",
        "\n",
        "                if best<=100*correct/total:\n",
        "\n",
        "                    save_path = model_path\n",
        "                    os.makedirs(save_path, exist_ok=True)\n",
        "                    torch.save(model.state_dict(), save_path + '/trained_model.pt')\n",
        "                    best = 100*correct/total\n",
        "\n",
        "                print('Epoch: %d/%d, Tr.loss: %.6f, Val.loss: %.6f, Val.Acc.: %.2f, Best Acc.: %.2f'%(epoch+1, num_epochs, train_loss.item(), 100 *test_loss/total, 100*correct/total,best))\n",
        "    return best"
      ],
      "metadata": {
        "id": "xmZ5nZCd2hVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureExtractor(nn.Module):\n",
        "    def __init__(self,in_channel, num_features):\n",
        "        super().__init__()\n",
        "        self.pool = SelectAdaptivePool2d(1,pool_type='avg')\n",
        "        self.norm = LayerNorm2d(in_channel)\n",
        "        self.fc = nn.Linear(in_channel,num_features)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, (nn.Linear)):\n",
        "            nn.init.xavier_normal_(m.weight)\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.pool(x)\n",
        "        x = self.norm(x)\n",
        "        x = torch.flatten(x,1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "aU4_NsCn7Nu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InputFilter(nn.Module):\n",
        "    def __init__(self,in_channel, out_channel):\n",
        "        super().__init__()\n",
        "        self.upsampling = nn.Upsample(size=(56, 56), mode='bilinear', align_corners=True)\n",
        "        self.conv = nn.Conv2d(in_channel,out_channel,kernel_size=1)\n",
        "        self.norm = LayerNorm2d(out_channel)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, (nn.Conv2d)):\n",
        "            nn.init.xavier_normal_(m.weight)\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.upsampling(x)\n",
        "        x = self.conv(x)\n",
        "        x = self.norm(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "RcYNmxX67QEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        model = timm.create_model('convnext_tiny',\n",
        "                                  pretrained=True,\n",
        "                                  num_classes=2)\n",
        "\n",
        "        self.stem = InputFilter(56,96)\n",
        "        self.stages = model.stages\n",
        "        self.clf = model.head\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # input\n",
        "        x = self.stem(x)\n",
        "        x = self.stages(x)\n",
        "        x_out = self.clf(x)\n",
        "\n",
        "\n",
        "        return x_out"
      ],
      "metadata": {
        "id": "-KcvVKp77UHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed = 0):\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.enabled = False\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "seed_num = 0\n",
        "set_seed(seed_num)"
      ],
      "metadata": {
        "id": "FqSuPJFa1-A8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"CUDA_DEVICE_ORDER\"] = 'PCI_BUS_ID'\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
        "loss_function = nn.CrossEntropyLoss().cuda()\n",
        "model_path = 'model_trained/proposed'\n",
        "\n",
        "model = Model()\n",
        "model.cuda()\n",
        "\n",
        "save_path = model_path\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(),lr=1e-4, weight_decay=1e-5)"
      ],
      "metadata": {
        "id": "kAzuPUmF2MIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = 'Data/classification/'\n",
        "params = {\n",
        "    'num_epochs': 100,\n",
        "    'batch_size': 64,\n",
        "    'seed_num':seed_num,\n",
        "    'optimizer':optimizer,\n",
        "    'loss_function':loss_function,\n",
        "    'data_path': data_path,\n",
        "    'model_path': model_path,\n",
        "    'acc_best': 0,\n",
        "    'norm': 0,\n",
        "    'lambda':1e-3\n",
        "    }\n",
        "\n",
        "train(model, params)\n",
        "\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "WtH-HsbN7tHj",
        "outputId": "e96c62c5-e2d3-44dc-ef98-fdf901f83e52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-ee6d1bb780f9>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     }            \n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-24b869a20711>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, params)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0ml_lambda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lambda'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mds_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mds_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-07f8f4990139>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, mode)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/splits/{}.txt'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0mdata_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Data/classification//splits/train.txt'"
          ]
        }
      ]
    }
  ]
}